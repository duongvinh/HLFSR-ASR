import time
import argparse
from torch.autograd import Variable
import torch.backends.cudnn as cudnn
from utils.utils import *
from model import Net
from tqdm import tqdm

from model_HLFSR_ASR import HLFSR_ASR

# Settings
def parse_args():
	parser = argparse.ArgumentParser()
	parser.add_argument('--device', type=str, default='cuda:0')
	parser.add_argument('--parallel', type=bool, default=False)
	parser.add_argument('--num_workers', type=int, default=8)

	parser.add_argument("--angRes_in", type=int, default=2, help="input angular resolution")
	parser.add_argument("--angRes_out", type=int, default=7, help="output angular resolution")
	parser.add_argument('--trainset_dir', type=str, default='../Data/TrainData_Lytro_2x2-7x7/')
	parser.add_argument('--testset_dir', type=str, default='../TestData_Lytro_2x2-7x7/')
	parser.add_argument('--model_name', type=str, default='HFLSR_ASR_Lytro_2x2-7x7')

	parser.add_argument('--batch_size', type=int, default=4)
	parser.add_argument('--lr', type=float, default=2e-4, help='initial learning rate')
	parser.add_argument('--n_epochs', type=int, default=70, help='number of epochs to train')
	parser.add_argument('--n_steps', type=int, default=15, help='number of epochs to update learning rate')
	parser.add_argument('--gamma', type=float, default=0.5, help='learning rate decaying factor')

	parser.add_argument('--crop', type=bool, default=True, help="LFs are cropped into patches for validation")
	parser.add_argument("--patchsize", type=int, default=128, help="LFs are cropped into patches for validation")
	parser.add_argument("--stride", type=int, default=64, help="LFs are cropped into patches for validation")
	
	parser.add_argument('--load_pretrain', type=bool, default=False)
	parser.add_argument('--model_path', type=str, default='./log/HFLSR_ASR_Lytro_2x2-7x7.pth.tar')
	parser.add_argument("--n_groups", type=int, default=5, help="number of HLFSR-Groups")
	parser.add_argument("--n_blocks", type=int, default=15, help="number of HLFSR-Blocks")
	parser.add_argument("--channels", type=int, default=32, help="number of channels")

	return parser.parse_args()


def train(cfg, train_loader, test_Names, test_loaders):
	if cfg.parallel:
		cfg.device = 'cuda:0'

	net = HLFSR_ASR(angRes_in=cfg.angRes_in, angRes_out=cfg.angRes_out, n_blocks=cfg.n_blocks,
				   channels=cfg.channels)

	net.to(cfg.device)
	cudnn.benchmark = True
	epoch_state = 0

	total_params = sum(p.numel() for p in net.parameters())
	print("Total Params: {:.2f}".format(total_params)) 

	if cfg.load_pretrain:
		if os.path.isfile(cfg.model_path):
			model = torch.load(cfg.model_path, map_location={'cuda:0': cfg.device})
			net.load_state_dict(model['state_dict'])
			epoch_state = model["epoch"]
		else:
			print("=> no model found at '{}'".format(cfg.load_model))

	if cfg.parallel:
		net = torch.nn.DataParallel(net, device_ids=[0, 1])

	criterion_Loss = torch.nn.L1Loss().to(cfg.device)
	optimizer = torch.optim.Adam([paras for paras in net.parameters() if paras.requires_grad == True], lr=cfg.lr)
	scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=cfg.n_steps, gamma=cfg.gamma)
	scheduler._step_count = epoch_state
	loss_epoch = []
	loss_list = []

	for idx_epoch in range(epoch_state, cfg.n_epochs):
		for idx_iter, (data, label) in tqdm(enumerate(train_loader), total=len(train_loader)):
			data, label = Variable(data).to(cfg.device), Variable(label).to(cfg.device)
			out = net(data)
			loss = criterion_Loss(out, label)
			optimizer.zero_grad()
			loss.backward()
			optimizer.step()
			loss_epoch.append(loss.data.cpu())

		if idx_epoch % 1 == 0:
			loss_list.append(float(np.array(loss_epoch).mean()))
			print(time.ctime()[4:-5] + ' Epoch----%5d, loss---%f' % (idx_epoch + 1, float(np.array(loss_epoch).mean())))
			if cfg.parallel:
				save_ckpt({
				'epoch': idx_epoch + 1,
				'state_dict': net.module.state_dict(),
			}, save_path='./log/', filename=cfg.model_name + '.pth.tar')
			else:
				save_ckpt({
					'epoch': idx_epoch + 1,
					'state_dict': net.state_dict(),
				}, save_path='./log/', filename=cfg.model_name + '.pth.tar')

			loss_epoch = []

		''' evaluation '''
		with torch.no_grad():
			psnr_testset = []
			ssim_testset = []
			for index, test_name in enumerate(test_Names):
				test_loader = test_loaders[index]
				psnr_epoch_test, ssim_epoch_test = valid(test_loader, net)
				psnr_testset.append(psnr_epoch_test)
				ssim_testset.append(ssim_epoch_test)
				print(time.ctime()[4:-5] + ' Dataset----%15s, PSNR---%f, SSIM---%f' % (test_name, psnr_epoch_test, ssim_epoch_test))
				pass
			pass

		scheduler.step()
		pass


def valid(test_loader, net):
	psnr_iter_test = []
	ssim_iter_test = []
	for idx_iter, (data, label) in (enumerate(test_loader)):
		data = data.squeeze().to(cfg.device)  # numU, numV, h*angRes, w*angRes
		label = label.squeeze().to(cfg.device)
		if cfg.crop == False:
			with torch.no_grad():
				outLF = net(data.unsqueeze(0).unsqueeze(0).to(cfg.device))
				outLF = outLF.squeeze()
		else:
			uh, vw = data.shape
			h0, w0 = uh // cfg.angRes_in, vw // cfg.angRes_in
			subLFin = LFdivide(data, cfg.angRes_in, cfg.patchsize, cfg.patchsize // 2)  # numU, numV, h*angRes, w*angRes
			numU, numV, H, W = subLFin.shape
			subLFout = torch.zeros(numU, numV, cfg.angRes_out * cfg.patchsize, cfg.angRes_out * cfg.patchsize)

			for u in range(numU):
				for v in range(numV):
					tmp = subLFin[u, v, :, :].unsqueeze(0).unsqueeze(0)
					with torch.no_grad():
						torch.cuda.empty_cache()
						out = net(tmp.to(cfg.device))
						subLFout[u, v, :, :] = out.squeeze()
			outLF = LFintegrate(subLFout, cfg.angRes_out, cfg.patchsize, cfg.stride, h0, w0)

		psnr, ssim = cal_metrics_RE(label, outLF, cfg.angRes_in, cfg.angRes_out)

		psnr_iter_test.append(psnr)
		ssim_iter_test.append(ssim)
		pass

	psnr_epoch_test = float(np.array(psnr_iter_test).mean())
	ssim_epoch_test = float(np.array(ssim_iter_test).mean())

	return psnr_epoch_test, ssim_epoch_test


def save_ckpt(state, save_path='./log', filename='checkpoint.pth.tar'):
	torch.save(state, os.path.join(save_path, filename))


def main(cfg):
	train_set = TrainSetLoader(dataset_dir=cfg.trainset_dir)
	train_loader = DataLoader(dataset=train_set, num_workers=12, batch_size=cfg.batch_size, shuffle=True)
	test_Names, test_Loaders, length_of_tests = MultiTestSetDataLoader(cfg)
	train(cfg, train_loader, test_Names, test_Loaders)


if __name__ == '__main__':
	cfg = parse_args()
	main(cfg)
